name=newrelic-events-sink-connector

connector.class=com.newrelic.telemetry.events.EventsSinkConnector

# configure this based on your workload
tasks.max=1

topics=newrelic-events-sink-topic
api.key=<api.key>
nr.region=US
#nr.client.timeout        # Time, in milliseconds, to wait for a response from the New Relic API (default is 2000)|
#nr.client.proxy.host     # Proxy host to use to connect to the New Relic API |
#nr.client.proxt.port     # Proxy host to use to connect to the New Relic API (required if using a proxy host) | 
#nr.flush.max.records     # The maximum number of records to send in a payload. (default: 1000) |
#nr.flush.max.interval.ms # Maximum amount of time in milliseconds to wait before flushing records to the New Relic API. (default: 5000) |


# messages are stored in schemaless json on the topic
# you could use Avro, Protobuf, etc here as well
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=false
value.converter.schemas.enable=false

# declare the transformations
#transforms=inserttimestamp,eventtype,flatten

#Insert the timestamp from the Kafka record
#transforms.inserttimestamp.type=org.apache.kafka.connect.transforms.InsertField$Value
#transforms.inserttimestamp.timestamp.field=timestamp

# we know all events on this topic represent a purchase, so set 'eventType' to 'purchaseEvent'
#transforms.eventtype.type=org.apache.kafka.connect.transforms.InsertField$Value
#transforms.eventtype.static.field=eventType
#transforms.eventtype.static.value=Purchase

# flatten all nested json fields, using . as a delimeter
#transforms.flatten.type=org.apache.kafka.connect.transforms.Flatten\$Value
#transforms.flatten.delimiter=.